---
title: "人工知能の応用における倫理やガバナンス面の課題―第1部のメモ"
date: 2019-04-02T00:00:00
permalink: https://joi.ito.com/jp/archives/2019/04/02/005686.html
language: jp
categories: []
author: Joichi Ito
source_path: /jp/archives/2019/04/02/005686.html
extracted: 2025-12-25
---

Jonathan Zittrainと共同で授業をするのは今回で三度目。今年の講義は、Applied Ethical and Governance Challenges in Artificial Intelligence（人工知能の応用における倫理やガバナンス面の課題）という。セミナー形式なので、講演者を招き、彼らの論文や研究について話し合うスタイルが基本。講演者や論文の選定は優秀なティーチング・アシスタント陣のSamantha Bates、John Bowers、Natalie Satielが担当した。

Samは授業前の準備として論文の概要を書き、進行を決めておくことも担当している。この作業が、講師たちが目を通すメモという形で終わってしまうのはもったいないと思った。メモや概要を僕のブログに掲載すれば、誰でも授業内容の一部を習得できるし、興味深い会話のきっかけにもなるかもしれないのでSamの承諾を得て載せることにした。

この講義は、3つのテーマについて3回授業を行うセットを3回行う構成となっている。以前の授業は、トピックの全般的な概要に近い内容だったけれど、研究が進むうちに、多くの人が既に知っていることを復習するよりは、肝心な事柄を掘り下げたほうが面白い、という皆の意見が一致した。

選んだトピックはfairness（公平性）、interpretability（解釈可能性）、adversarial examples（敵対的事例）の３つ。各トピックに3回授業を割り当て、"診断"（問題の技術的な根本を同定する）、"予測"（問題の社会的影響を探求する）、"介入"（同定した問題の解決策を挙げ、各案のコストと利点を考慮しながら検討する）という順番で取り上げる。構成図は下記参照。

生徒たちはMITとハーバードから半々の割合で来ていて、彼らの専門分野はソフトウェア工学、法律、政策など幅広い。授業はとても良い形で進んでいて、いろんなトピックについて、こんなに深く学んだのは初めてだ、と個人的に感じている。その反面、諸問題が如何に難解かが明らかになってきて、これらのアルゴリズムの展開によって社会にもたらされる危害を最小限に留めるために必要な取り組みが、あまりにも規模が膨大なため、途方に暮れそうな時もある。

ちょうど"予測"の段階が終わったところで、これから"介入"を始めるところだ。次のステージに突入するにあたって、希望が持てる要素を見つけられればと思う。

以下、序論と第1段階（"診断"）の概要とシラバスをSamantha Batesがまとめたもの。論文へのリンクも掲載する。

第1段階を手短にまとめると、「公平性」をどう定義するかは不明で、特定な式や法則として表現するのは多分不可能だけど、動的なものである、ということは言える。「解釈可能性」は聞こえの良い言葉だけど、授業でZachary Liptonが言ったように、wastebasket taxon（くずかご的な分類群）であり、例えるならアンテロープに似た動物が実際にアンテロープかどうかにかかわらず、すべて「アンテロープ」と呼ばれているのに似た使い方をされる言葉だ。「敵対的事例」については、MITの数名の生徒たちが、我々は敵対的攻撃に対処する準備ができておらず、これらの攻撃に対して堅固でありながら効果的に機能するアルゴリズムを構築できるかどうかは不明、ということを明確に示してくれた。

第1部：序論と診断

Samantha Bates作

![](https://lh3.googleusercontent.com/mbxEwNyXALP8zp3eT85A1SAkUzbzNL28CGygZ1S6yoYxBhSclO-PpX4kID5j0mapz2efUy4EOl_g5uTJtbg-ISe5QiV5KWmlug-reMn1jSAW4pyievAPG0CtNXISIrsj--Dv75PB)

このテーマについて初めての投稿となる今回のブログでは、第４回までの授業の宿題として課された読み物をまとめており、序論から第1段階までが含まれている。"診断"段階では、クラス全員で公平性、解釈可能性、敵対的事例に関するAIの主要問題を同定し、自律システムの根本的なメカニズムがどのようにそれらの問題に関与しているかを検討した。授業での話し合いは、用語の定義やテクノロジーの仕組みなどを中心に行なった。講義シラバスの第1部と各読書物の要点をまとめたメモを以下に掲載する。

第1回：序論

1回目の授業では、講義の構成と目的を提示し、後の議論に向けて、この分野の現状を批評した読書物を宿題として出した。

"[Artificial Intelligence -- The Revolution Hasn't Happened Yet](https://perma.cc/4JM9-E4HC)" by Michael Jordan, Medium (April 2018)

Michael Jordan（2018年4月）．人工知能―革命はまだ起きていない．Medium

"[Troubling Trends in Machine Learning Scholarship](https://perma.cc/H6Q9-HZZD)" by Zachary C. Lipton & Jacob Steinhardt (July 2018)

Zachary C. Lipton、Jacob Steinhardt（2018年7月）．機械学習の学問における不穏な傾向

上記の論文は両方とも現行のAI研究や議論に批判的な内容だけど、それぞれ違う視点から書かれている。Michael Jordanの論文の要点は、AI研究において多様な学問分野間の連携が不足していること。工学の新しい分野が誕生している今、非技術的な課題や視点も取り入れる必要がある、という主張だ。『Troubling Trends in Machine Learning Scholarship』（機械学習の学問における不穏な傾向）は、学問としての機会学習のコミュニティーにおいて、基準が低下しており、研究手法や慣行が厳格さに欠けていることに焦点を当てている。両方の論文において、著者たちは、この分野への信頼が保たれるよう、学問において厳格な基準が守られることを義務付けるべきだ、という正しい指摘をしている。

最初の読み物を、この分野の現状を批評する内容の論文にしたのは、生徒たちがこの講義で読むことになる諸論文を、客観的・論理的な視点から考えるように促すため。混乱を避けるためには正確な用語の使用や思考の説明が特に重要であることをこれらの論文が示してくれるのと同じように、生徒たちには、自分の研究や意見をどう伝えるか、慎重に検討するように、と私たち講師は求めている。この初回の読み物は、これから特定なトピック（公平性、解釈可能性、敵対的ＡＩ）を深く掘り下げる状況を整えてくれるものであり、講義で議論する研究についてどのようなアプローチで臨むべきか生徒たちに理解してもらうのに役立つのだ。

第２回：公平性に関する問題を診断する

診断ステージの1回目の授業では、機械学習における公平性に関する指導的発言者として活躍中のデータ科学者で活動家のCathy O'Neilを講演者に迎えた。

Weapons of Math Destruction by Cathy O'Neil, Broadway Books (2016). Read Introduction and Chapter 1: "Bomb Parts: What Is a Model?"

Cathy O'Neil（2016）．Weapons of Math Destruction．Broadway Books

序論と第1章：爆弾の部品―モデルとは何か？

[OPTIONAL] "[The scored society: due process for automated predictions](https://perma.cc/8QD4-JQAS)" by Danielle Keats Citron and Frank Pasquale, Washington Law Review (2014)

（任意）Danielle Keats Citron、Frank Pasquale（2014）．スコア付けされた社会：自動化された予測の正当な手続き．Washington Law Review

Cathy O'Neilの著書『Weapons of Math Destruction』は、予測モデルとその仕組み、そして予測モデルに偏りができてしまう過程などが分かりやすく書かれた入門書だ。欠陥のあるモデルが不透明で拡張性を持ち、生活に損害を与えてしまう場合（貧困層や社会的弱者が被害を受ける場合が多い）を、彼女はWeapons of Math Destruction (WMD)と呼ぶ。善意があっても、信頼性のある結論を出すために必要な量のデータが不足していたり、欠けているデータを代用品で補ったり、単純過ぎるモデルで人間行動を理解し、予測しようとする場合、WMDができやすい、とO'Neilは言う。人間行動は複雑で、少数の変数で正確に模型を作れるものではない。厄介なのは、これらのアルゴリズムはほとんどの場合、不透明なため、これらのモデルのあおりを受ける人は対抗することができない点だ。

O'Neilは、このようなモデルの採用が、予期し得ない深刻な結果をもたらす場合があることを示している。WMDは人間による検討や決断に代わる安価な手段であるため、貧困地域で採用されやすく、そのため、貧困層や社会的弱者へより大きな影響を与える傾向にある。また、WMDは行動の悪化をもたらす場合もある。O'Neilが挙げたワシントンD.C.のある学区の例では、結果が出せない教員を特定し、排除するために生徒たちのテスト成績を使ったモデルが採用されており、仕事を失わないために生徒のテスト成績を改ざんする教員もいたのだ。この場合、WMDは教員の質向上を目的に採用されたにもかかわらず、意図せぬ行動を奨励する構造ができてしまったため、逆の効果をもたらしてしまった。

任意の読み物『The Scored Society: Due Process for Automated Predictions』は、金融における信用スコアリングに関するアルゴリズムの公平性に関する論文だ。Cathy O'Neilがしたように、著者たちは信用スコアリングのアルゴリズムは既存の社会的不平等を悪化させていると指摘し、法制度には現状を変える責任があると主張している。信用スコアリングや信用情報の共有プロセスを公にすべき、と著者たちは提唱していて、スコアを左右する項目について信用スコアリング会社が一般人向けに教育を行なうことを義務化すべきともしている。Cathy O'NeilがWMDの３つの特徴のひとつとして挙げた不透明さの問題を改善すれば、信用スコアリング制度をより公平なものにし、知的財産権の侵害やスコアリング・モデルの廃止を回避できる、と著者たちは言う。

第３回：解釈可能性に関する問題を診断する

3回目の授業では、機械学習における解釈可能性の問題の定義と対処に取り組んでいるCarnegie Mellon UniversityのZachary Lipton助教授を迎え、解釈可能性のあるモデルとはどういうものか、について話し合った。

"[The Mythos of Model Interpretability](https://perma.cc/6MV7-HLHB)" by Zachary C. Lipton, ArXiv (2016)

Zachary C. Lipton（2016）．モデルの解釈可能性という神話．ArXiv

[OPTIONAL] "[Towards a rigorous Science of Interpretable Machine Learning](https://perma.cc/U7FF-NPZ5)" by Finale Doshi-Velez and Been Kim, ArXiv (2017)

（任意）Finale Doshi-Velez、Been Kim（2017）厳格な科学としての解釈可能性のある機械学習に向けて．ArXiv

3回目の授業は解釈可能性について話し合う最初の日だったので、この日のための読み物は両方とも「解釈可能性」をどう定義すべきか、そして解釈可能性が重要な理由を内容とするものを選んだ。Liptonの論文は、「解釈可能性」とは複数のアイデアを反映するものであり、現行の定義は単純過ぎる場合が多いと主張する。この論文は、議論のお膳立てをする質問を挙げている。「解釈可能性」とは何か？「解釈可能性」が最も必要となるのはどんな背景や状況か？より透明性が高いモデルや、成果を説明できるモデルを作れば、それは解釈可能性のあるモデルとなるのか？

これらの質問を検討することによってLiptonは、解釈可能性の定義はモデルに解釈可能性を望む理由によって変わる、と主張する。モデルに解釈可能性を求めるのは、その根底にあるバイアスを同定し、アルゴリズムの影響を受けてしまう人がその成果に異議を唱えられるようにするためかもしれない。あるいは、アルゴリズムに解釈可能性を求めるのは、決定に係る人間により多くの情報を提供できるようにして、アルゴリズムの正当性を高める、あるいは要因間に潜んでいるかもしれない因果関係を明らかにし、さらに検証できるようにするためかもしれない。我々が解釈可能性を求めているのはどんな状況においてか、諸々の状況の違いを明確にすることによって、解釈可能性が持つ多様な側面をより正確に反映する仮の定義に近づくことができる、とLiptonは言う。

さらに、Liptonは解釈可能性を向上させるために二種類の提案を検討している。透明性を高めることと、事後説明を提供すること。透明性を高めるアプローチは、モデル全体に適用される場合がある（シミュレーション可能性）。つまり、同じ入力データとパラメータがあれば、ユーザーはモデルの成果を再現できるはず。また、透明性を高める方法として、モデルの各要素（入力データ、パラメータ、計算）に個別に解釈可能性を持たせる方法や、トレーニング段階では、トレーニング・データセットがどんなものであれ、そのモデルは独自のソリューションにたどり着くことを示す方法もある。しかし、介入段階で更に詳しく述べるように、各レベルの透明性を増やすことは、前後関係や採用されるモデルの種類によって、必ずしも得策ではない（例えば、リニア・モデルに対するニューラル・ネットワーク・モデル）。また、モデルの透明性を向上させることは、そのモデルの正確性や効力の低下につながる場合もある。解釈可能性を向上させる二つ目の方法として、事後解釈可能性を義務付ける方法がある。つまり、成果を出した後、そのモデルは意思決定プロセスを説明しなければならない、という仕組みにするのだ。事後説明は文字、映像、サリエンシー・マップ、あるいは似たような状況において似たような決断が下された経緯を示す類推などの形式で行なうことができる。事後説明は、モデルの影響を受けた人間がどうすればその成果に対抗したり、その成果を変えることができるかについて洞察を与えてくれる場合もあるけれど、こういった説明は意図せず誤解を招くこともあり、人間のバイアスに影響されている場合は特にその傾向が強い、とLiptonは警告する。

Liptonの論文の結論は、解釈可能性を定義することは非常に難しい、というもので、その理由として、前後関係や、モデルに解釈可能性を持たせる動機など、外的要因によって定義が大きく変わることが挙げられる。解釈可能性という用語の仮の定義が無い限り、モデルに解釈可能性があるかどうか判断する方法は不明のままである。Liptonの論文は解釈可能性をどう定義するか、そして解釈可能性は何故重要か、ということに焦点をあてているが、任意の読み物『Towards a rigorous Science of Interpretable Machine Learning』は、モデルに解釈可能性があるかどうかを判断するための様々な方法をより深く掘り下げて調べている。著者たちは解釈可能性を「人間に理解できる言葉で説明あるいは提示する能力」と定義しており、解釈可能性を評価する基準が無いことに特に懸念を抱いている。

第４回：敵対的事例への脆弱性を診断する

敵対的事例に関する一回目の授業では、敵対的テクニックについて最先端の研究を行っているMITの学生主導研究会LabSixを迎えた。[LabSix](https://www.labsix.org/)は敵対的事例の初歩を説明し、彼らの研究の発表もしてくれた。

"[Motivating the Rules of the Game for Adversarial Example Research](https://perma.cc/3ZQQ-A7MY)" by Justin Gilmer et al., ArXiv (2018).

Justin Gilmerら（2018）．敵対的事例の研究に関するルールを動機付ける．ArXiv

[RECOMMENDED] "Intriguing properties of neural networks" by Christian Szegedy et al., ArXiv (2013)

（推奨）Christian Szegedyら（2013）．ニューラルネットワークの興味深い性質．ArXiv

Gilmerらの論文は、敵対的事例を分かりやすく紹介する読み物であり、敵対的事例の定義を「機械学習モデルに間違いを起こさせるために攻撃者が意図的に設計した入力」としている。この論文の趣旨は、攻撃者が敵対的事例を採用する可能性のある様々なシナリオを検証することにある。著者たちは、攻撃の種類を整理すべく、分類用語集を作成しており、次の用語が挙げられている。「indistinguishable perturbation（識別不能な摂動）、content-preserving perturbation（コンテンツ保存型摂動）、non-suspicious input（疑わしくない入力）、content-constrained input（コンテンツに制約された入力）、unconstrained input（制約のない入力）」。それぞれの攻撃カテゴリーについて、著者たちは攻撃者の動機や制約を検討している。様々な種類の攻撃やそれぞれの代償を理解することによって、機械学習システムの設計者は防御能力を高めることができる、と著者たちは主張する。

この論文には摂動に対する防御に関する文献の概要も含まれており、これまでの文献では、実際にありそうな、実世界における状況での敵対的事例攻撃が検討されていない、と著者たちは批判している。例えば、防御に関する文献において頻繁に挙げられる仮定の状況として、攻撃者が自動走行車を混乱させるために止まれ標識の映像を摂動する場合がある。しかし、Gilmerらは、この車のエンジニアたちは、システム自体による、あるいは実世界の出来事に起因した誤分類エラー（例えば、止まれ標識が風の影響で倒れた場合）を想定し、対策を準備したはずだと指摘する。攻撃者が車を混乱させる方法として、より簡単で非技術的なやり方があり、前述の仮定よりも現実的なテスト・ケースがあるはずだ、と著者たちは主張する。防御に関する文献について、著者たちによるもう一つの批判は、システムの防御体制のとある面を改善すると、そのシステムの他の面の頑丈さが低下してしまい、攻撃への脆弱性を高めてしまう場合があることを取り上げていない、というものだ。

任意の読み物として推奨したChristian Szegedyらの論文は、内容がより専門的で、用語をすべて理解するには機械学習の知識が必要である。難易度の高い読み物だけど、「敵対的事例」という用語を提唱し、このトピックに関する研究の基礎の構築に貢献した論文なのでシラバスに含めることにした。

###### Credits

Notes by Samantha Bates

訳：永田 医