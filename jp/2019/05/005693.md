---
title: "人工知能の応用における倫理やガバナンス面の課題　第2部　予測"
date: 2019-05-25T00:00:00
permalink: https://joi.ito.com/jp/archives/2019/05/25/005693.html
language: jp
categories: []
author: Joichi Ito
source_path: /jp/archives/2019/05/25/005693.html
extracted: 2025-12-25
---

Applied Ethical and Governance Challenges in Artificial Intelligence（人工知能の応用における倫理やガバナンス面の課題）という授業をJonathan Zittrainと共同で行なっている。ティーチングアシスタントのSamantha Batesがまとめたシラバスや概要を3回に渡ってブログ投稿する予定で、今回が2回目。John BowersとNatalie Satielもティーチングアシスタントを担当している。[1回目のポストはこちら。](https://joi.ito.com/jp/archives/2019/04/02/005686.html)

僕なりに要点をまとめてみた。

第1部では、この分野の定義付けを行い、いくつかの課題を理解しようとしてみた。この分野における文献の多くでは、公平性と説明可能性は曖昧な定義による単純化し過ぎる議論で語られており、懸念が残る形で終わった。また、敵対的攻撃や類似するアプローチなどの新しいリスクに対して、技術に携わるコミュニティーとして我々はどう対応すればいいのか、今後の難題に危惧しながら第1部を終えた。

第2部では、人工知能における倫理とガバナンスについて、ある種の絶望感を抱えながら、課題をさらに深く追求していく。Solon BarocasとAndrew D. Selbstが共著した論文『Big Data's Disparate Impact』（ビッグデータの差別的な影響）では、アメリカの公民権法第７編（タイトル・セブン）を取り上げ、差別や公平性に関する法律の現状が紹介されている。この法律は、公民権運動で提起された差別問題を是正すべく制定されたものだったが、司法制度はアファーマティブ・アクション（積極的格差是正措置）などの救済手段を通して社会的不公平を正す方向から離れてしまった、と著者たちは言う。代わりに、法制度はプロセスの公平性に重点を置き、所得の再分配や歴史的不公平の解決を軽視するようになった。その結果、法制度は公平性についてより厳密な見解を持つようになり、いわば保険数理的な「all lives matter」（黒人差別に反対する社会運動Black Lives Matterに対抗するスローガン）的なアプローチとなっている。第1部で[アマゾンの偏った雇用ツール](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)について話し合った際、人為的な調整によって女性やマイノリティのスコアを増やせばいいのでは？という解決策も提案された。BarocasとSelbstの論文では、このような解決方法はもう法律では支持されていないことが紹介されている。エンジニアたちは「差別を禁止する法律は当然あるはずだから、それを使おう」と思ったようだ。実際は、その法律は所得の再分配や社会的公平性について飽きらめているのだ。Jonathanは、不法行為法における社会的不平等の扱いも似たようなものだと指摘する。例えば、交通事故で裕福な人と貧乏な人を同時に轢いてしまった場合、裕福な遺族により多く賠償金を払う必要があるのだ。賠償金の計算は、被害者の将来の収益力に基づく。不法行為法では、タイトル・セブンと同じように、「社会的な不公平は存在するかもしれないが、この法律はその問題を解決するものではない」ということになっている。

Sandra Wachterの論文では説明可能性を提供する方法としてcounterfactual（反事実的条件）の活用が提案されている。これは素晴らしいアイデアで、説明可能性に関する議論を前に進めることができるものだと思う。ただし、GDPRなどの法律によってそのような説明の提供を企業に義務付けることが実際に可能か、Sandraも懸念を抱いているようだ。僕たちもcounterfactualの限界について、バイアスの特定や、個人に応じた"最高"の答えを出すことができるのか、いくつか懸念を感じている。この限界はSandraも論文で取り上げている。

最後に、敵対的攻撃について理論的なアプローチからさらに進めて具体例を検証するために、医療系AIシステムに対する敵対的攻撃のリスクに関してJonathanと僕がJohn Bowers、Samuel Finlayson、Andrew L. Beam、Isaac S. Kohaneと共著し、[最近発表した論文](http://science.sciencemag.org/content/363/6433/1287)を取り上げた。

第2部を構成する3回の授業については、Samanthaが作成したまとめや読み物へのリンクも掲載するので、参照されたし。

**第2部：予測**

Samantha Bates作

**シラバス・メモ："予測"段階**

『人工知能おける倫理やガバナンス面の課題』のシラバスの第2部へようこそ！第1部では、宿題として課された読み物や授業での話は、自律システムの社会的、技術的、そして哲学的なルーツがいかにして公平性、解釈可能性、そして敵対的事例に関する問題に関与しているかを理解することに焦点を置いた。この講義の第2ステージは"予測ステージ"と位置づけ、これらの問題の社会的を検討する内容とした。このステージで最も重要だったのは、これらの問題の多くは社会的や政治的な問題であり、法律や技術によるアプローチのみで対応するのは不可能、ということが明らかになった点かもしれない。

**5回目の授業：不公平なAIの影響を予測する**

予測ステージの初日には、Cornell UniversityのSolon Barocas助教授が授業に参加。雇用におけるアルゴリズムの利用について法律や技術の観点から検証した同氏の論文『Big Data's Disparate Impact』(ビッグデータの差別的な影響)について話し合った。

- "[Big Data's Disparate Impact](https://perma.cc/YFW7-36L7)" by Solon Barocas and Andrew D. Selbst, California Law Review (2016) / Solon Barocas、Andrew D. Selbst（2016年）．ビッグデータの差別的な影響．California Law Review

予測ステージの初日では、クラスの焦点が、自律システムの根底にある技術的な仕組みを検討することから、それらの制度の社会的な影響を検証することに変わった。BarocasとSelbstの論文はデータ収集やデータのラベル付けが既存の偏見を意図的にも非意図的にも永続させている場合があることについて考察している。著者たちはデータ・セットに差別的な効果がある主な例を5つ紹介している。

1. 自律システムによる決定に利用されるパラメータを、データ・マイニングを行なう人間が決めるとき、私たち自身の人間的なバイアスがデータセットに組み込まれる可能性がある。
2. トレーニング・データが収集された方法やレベル付けされた方法によっては、すでに偏りがある可能性がある。
3. データ・マイニング・モデルは限られた数のデータ要素を検討しているため、扱われているテーマにとって典型的でないデータに基づいて個人や集団に関する結論を出してしまう可能性がある。
4. Cathy O'Neilが言ったように、モデルが決断する際に利用するデータ要素が階級身分の代用物である場合、偏見が入ってしまう可能性がある。
5. データ・マイニングが差別的なのは意図的である可能性がある。ただし、意図的でない場合のほうが多く、意図的であるかどうかを特定するのも難しい、と著者たちは主張する。

雇用における差別の是正に取り組んでいる法原理は存在するものの、実際に適用することが難しいことを著者たちは明らかにしており、この傾向はデータ・マイニングにおいて特に強い。公民権法の第7編（タイトル・セブン）は意図的な差別（差別的取扱い）と非意図的な差別（差別的インパクト）に対して法的責任を定めているが、いずれの種類も立証するのは難しい。例を挙げると、意図的な差別を理由に雇用者に責任を負わせるには、原告は代わりになる非差別的な方法が存在し、差別的な慣行と同じ目的を達成しうることを示さなければならない。また、雇用者に代替手段が提示された際、雇用者がその検討を拒否したことも証明しなければならない。大抵の場合、雇用者側は、代替手段について認識していなかったことを証明できれば、あるいは、差別的な要素があるかもしれない方針に正当な業務上の理由（業務上の必要性に基づいた弁護）があれば、裁判での防御が成功するのである。

データ・マイニングにおけるバイアスを明らかにし、証明し、是正するのが非常に難しいのは、社会全体として、差別への対処における法律の役割をはっきりさせていない、ということも関係している。anticlassification theory（反分類理論）という説によれば、法制度には、意思決定者が社会の被保護階層を差別しないよう保証する義務があるのだ。これに対抗する理論のantisubordination theory（反服従理論）は、より現場主義的な取り組み方を推奨しており、法律制度は、社会から取り残された人々の生活を積極的に改善させることによって身分に基づいた不平等を社会レベルでなくすことに取り組むべきである、としている。現行の社会では反分類的な取り組みが支持されており、その理由として、反差別法は被保護階層がより良い機会を得られるようにすることのみを目的としていない、という主旨の判決が早い段階で下されて先例が確立されたことも関係している。著者たちは、データ・マイニングが如何にして雇用における既存のバイアスを悪化させるかを示しているものの、効率的な意思決定と偏りの排除を両立させようとしたとき、社会的な代償があるのだ。

この論文は、問題解決の責任は誰にあるのか、という問題も提起している。BarocasとSelbstは、データ・マイニングにおけるバイアスの大半は意図的でないと強調しており、バイアスを明らかにし、技術的な修正を導入することによって偏りを無くすのは非常に困難かもしれない、という。同時に、この問題を法制度において解決するのを同じぐらい困難にしている政治的や社会的な要因があり、この問題への取り組みは誰が担当すべきか？という問題もある。著者たちは、社会全体として、私たちは差別に関する諸問題への取り組み方を見直す必要があるかもしれない、としている。

**６回目の授業：解釈可能性が無いAIの影響を予測する**

6回目の授業では、弁護士でOxford Internet Instituteの研究員でもあるSandra Wachterを迎え、自律システムに解釈可能性を持たせるためにcounterfactual（反事実的条件）を利用する可能性について話し合った。

- "[Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR](https://perma.cc/TPX7-Y689)" by Sandra Wachter et al., Harvard Journal of Law and Technology (2018)
- [OPTIONAL] "[Algorithmic Transparency for the Smart City](https://perma.cc/EFQ3-MH9F)" by Robert Brauneis & Ellen P. Goodman, Yale Journal of Law and Technology (2018) /（任意）Robert Brauneis、Ellen P. Goodman（2018年）．知的な都市のためのアルゴリズムの透明性．Yale Journal of Law and Technology

解釈可能性に関する前回の話し合いでは、クラス全体として、「解釈可能性」という用語の定義は、決定の背景や前後関係、そしてモデルに解釈可能性を持たせる動機によって大きく変わるため、定義できない、という結論に達した。Sandra Wachterらの論文は、「解釈可能性」を定義することは重要ではなく、焦点を置くべきところは、個人がモデルの成果を変えたり、対抗するための手段を提供することだと主張する。著者たちは、これらの自動化システムをより透明性のあるものとし、システムに責任を取らせる方法を立案すれば、AIに対する一般人の信頼を高める結果をもたらす、と指摘しているが、論文の主な焦点は、GDPRの説明要件を満たす自律モデルを設計するにはどうすればいいか、というところにある。論文が提案する解決策は「ある決定が受け止められた理由と、その決定に反対する手段と、どうすれば望まれる結果を将来的に得られるかについて限られた"アドバイス"を提供する」counterfactualを個々の決定（ポジティブなものとネガティブなもの両方）に対して発生させることである。

CounterfactualはGDPRの説明可能性の要件を満たし、上回るだけでなく、法的拘束力のある説明義務に向けた土台を作る効果がある、と著者たちは主張する。自動化されたモデルの技術的な仕組みを一般人に説明する難しさや、企業秘密や知的財産を守ることに関連する法的課題、そしてデータの対象者のプライバシーを違反する危険により、AIによる意思決定に関する透明性をより多く提供することはこれまで困難だった。しかし、counterfactualはこれらの課題に対する回避手段となり得る。なぜならば、counterfactualは「入力値がこう違っていれば、決定もこう変わる」と説明するものであり、モデルの仕組みを開示するものではないからである。例を挙げると、銀行ローンのアルゴリズムに関するcounterfactualは、ローンを拒否された人物に対して、年収が3万ドルでなく、4万5千ドルだったらローンを受けることができた、と伝えるかもしれない。この例でのcounterfactualは、モデルの技術的な仕組みを説明せずに、当事者に決定の根拠と、将来的に結果を変えるにはどうすればいいかを伝えることができる。なお、counterfactualはバイアスや不公平が絡む問題への十分な解決策ではない。あるモデルにバイアスがあることの証拠提供なら、counterfactualにできるかもしれない。しかし、counterfactualはとある決定と特定な外的な事実との間の依存性を示すのみなので、偏りの原因かもしれないあらゆる要因を明らかにしたり、とあるモデルに偏りがないことを確認したりする働きは期待できない。

任意の読み物『Algorithmic Transparency for the Smart City』（知的な都市のためのアルゴリズムの透明性）は、市庁によるビッグデータ分析技術や予測アルゴリズムの使用に関する透明性を検証している。書類作成や情報開示の拙劣さや企業秘密に対する懸念が原因で、モデルがどのように機能したかや、結果として市に与えることとなる影響を理解するのに必要な情報を市庁が得られない状況が頻繁に起こった、と著者たちは結論付けている。この論文では、Watcher et al.の論文も言及する自律モデルを理解しようとしたときに直面する障害について考察をさらに発展させており、反事実的な説明の展開が適していそうなシナリオを複数提示している。

**７回目の授業：敵対的事例の影響を予測する**

予測をテーマとした3回目の授業では、敵対的事例に関するディスカッションの続きとしていくつかの起こり得るシナリオを検討し、特に、それらの利用が私たちにとって有利にも不利にもなり得る医療保険詐欺について話し合った。

- "[Adversarial attacks on artificial intelligence systems as a new healthcare policy consideration](https://www.sciencemagazinedigital.org/sciencemagazine/22_march_2019/MobilePagedArticle.action?articleId=1474639&app=false#articleId1474639)" by Samuel Finlayson, Joi Ito, Jonathan Zittrain et al., preprint (2019) / Samuel Finlayson、Joi Ito、Jonathan Zittrainら（2019年）．医療保険政策に関する新しい検討事項としてのＡＩシステムに対する敵対的攻撃．プレプリント
- "[Law and Adversarial Machine Learning](https://perma.cc/W46R-2GL7)" by Ram Shankar Siva Kumar et al., ArXiv (2018) / Ram Shankar Siva Kumarら（2018年）．法律と敵対的機械学習．ArXiv

敵対的事例を取り上げた前回の授業では、敵対的事例は如何にして作られるか、を理解するためのディスカッションが中心だった。今回の読み物は、敵対的事例が、利用方法によって私たちにとって有利にも不利にもなり得ることを掘り下げた内容となっている。論文『Adversarial attacks on artificial intelligence systems as a new healthcare policy consideration』（医療保険政策に関する新しい検討事項としてのＡＩシステムに対する敵対的攻撃）では健康保険費の不正処理に関する敵対的事例の利用を検証している。医師による「アップコーディング」という行為があり、これは、より多くの報酬を得るために、実際に行われた処置よりもはるかに重要な医療行為に対して保険金を請求することである、と著者たちは説明する。敵対的事例がこの問題を悪化させる場合が想定される。例えば、良性のほくろを写した画像に医師がわずかに手を加えた結果、保険会社の自律請求コード・システムが悪性のほくろとして誤って分類してしまう場合がある。保険会社は、保険請求の妥当性を裏付ける証拠の提出を追加で義務付けても、敵対的事例の利用によってそのシステムが騙されることもあり得る。

保険詐欺は医療におけて深刻な問題だが、その詐欺性が明確でない場合もある。また、医師がアップコーディングを行うのは、本来なら保険会社が認めない医薬品や治療を使えるようにして患者の医療体験をより良くするため、という場合もある。同様に、論文『Law and Adversarial Machine Learning』（法律と敵対的機械学習）は、機械学習の研究者に対して、彼らが構築する自律システムが、個人ユーザーにとって役に立つ場合もあれば、同じユーザーにとって悪影響が及ぶ使い方がされる場合もあることを検討すべきだとしている。研究者が作ったツールを、圧政的な政府が国民のプライバシーや言論の自由を侵害するために使う可能性もある、著者たちは研究者に警告している。同時に、圧政的な国家の下で生活している人々は、敵対的事例を利用して国家の顔認識システムを回避し、探知されないようにすることができるかもしれない。これらの例は両方とも、敵対的事例をどう扱うかは簡単に決められないことを示している。

これらの論文では、敵対的事例に起因した問題への介入策の作成に関する助言が記されている。医療においては、「procrastination principle」（先延ばしの原則）というインターネット初期に生まれた概念で、問題を未然に防ぐためにインターネットのアーキテクチャを変えるべきではない、とする説が、敵対的事例の場合にも当てはまるかもしれない、と著者たちは言う。早過ぎる段階で医療における敵対的事例に関する問題に取り組むと、効果的でない規制ができあがってしまい、この分野での革新の妨げとなる可能性がある、と著者たちは警告する。その代わりとして、敵対的事例に関する懸念については、既存の規制を延長し、保険金請求のために提出されるデータに対応する"指紋"的なハッシュ値を作成するなど、小さな段階を経て対応することを著者たちが提案している。

論文『Law and Adversarial Machine Learning』では、弁護士や為政者が、最善の機械学習政策を立てるには、機械学習の研究者の協力が必要である、と著者たちは強調する。従って、法律が解釈され得る場合やを法律をどう施行すべきかを為政者が理解するのを手伝うために、機械学習の開発者は敵対的事例のリスクを評価し、既存の防御システムの有効性を評価すべきである、と勧告している。機械学習の開発者はシステムを開発する際、攻撃が起きたかどうかの判定をはじめ、攻撃がどのように起きたかや、誰が攻撃を行なったかが判定しやすいシステム設計を心掛けるべきだ、と著者たちは提案する。例えば、「システムに対して敵対的攻撃が起きた際に警告を発し、適切な記録作りを勧告し、攻撃中の事件対応用の戦略を構築し、攻撃から回復するための復旧計画を立てる」システムを設計すれば対策になるだろう。最後に、著者たちは機械学習の開発者に対し、機械学習や敵対的事例は使い方によっては人権を侵害することもあれば、守るもできることに留意するよう呼び掛けている。

###### Credits

Notes by Samantha Bates, Syllabus by Samantha Bates, John Bowers and Natalie Satiel

訳：永田 医