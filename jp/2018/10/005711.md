---
title: "AIは未来を予測しない。いまを映す「鏡」である"
date: 2018-10-03T00:00:00
permalink: https://joi.ito.com/jp/archives/2018/10/03/005711.html
language: jp
categories: []
author: Joichi Ito
source_path: /jp/archives/2018/10/03/005711.html
extracted: 2025-12-25
---

掲載 [WIRED Insight: AIは未来を予測しない。いまを映す「鏡」である on 2018年9月 3日](https://wired.jp/2018/09/03/ideas-ai-as-mirror-not-crystal-ball/ "WIRED Insight: AIは未来を予測しない。いまを映す「鏡」である").

刑務所の収容者の数を減らすべきであるという点では、コーク兄弟［編註：米国の実業家で極めて大きな影響力がある］から米国最大の人権擁護団体である米国自由人権協会（ACLU）まで、誰もが意見が一致している。人種差別的かつ不公正なシステムが受刑者の増加を招いていると左派が批判する一方で、保守陣営は現行の刑事司法制度は非効率で改革が必要だと叫ぶ。ただ両者は少なくとも、塀のなかにいる人間の数を少なくするのはよいアイデアであるという点では、意見が一致している。

この問題をめぐっては、犯罪抑止に向けた取り組みを中心に、さまざまな場所で[人工知能](https://wired.jp/tag/ai)（[AI](https://wired.jp/tag/ai)）が活用されるようになっている。例えば、地域住民の人口構成や逮捕歴といったデータから犯罪の起こりやすそうな場所を特定するといったことが、実際に行われているのだ。また、保釈や仮釈放の決定を下す際のリスクの判定だけでなく、実際の刑罰の決定にまでAIのシステムが関与することもある。

改革派は政党を問わず、アルゴリズムを使えば人間より客観的なリスク評価が可能だと主張する。保釈の判断を例に考えてみよう。リスク判断において正確かつ効率的なシステムがあれば、勾留の必要がない容疑者を迅速に特定できる。

ところが、非営利の報道機関であるプロパブリカが2016年に行なった調査によると、こうしたシステムには人種的バイアスがかかっている。AIが黒人の保釈を高リスクと判定した比率は、[白人の2倍に達した](https://www.propublica.org/article/bias-in-criminal-risk-scores-is-mathematically-inevitable-researchers-say)というのだ。

わたしたちはアルゴリズムを、未来を予測する「水晶玉」のように考えている。しかし、実際には社会を批判的に見つめ直すための「鏡」ではないだろうか。

アルゴリズムは、わたしたちが気付かずにいる世のなかのひずみを正確に映し出す。機械学習やデータサイエンスを活用することで、貧困や犯罪を生み出している根本的な問題を解明することが可能になるかもしれない。しかしそれには、意思決定の自動化にこうしたツールを用いて不公正さを再生産するようなことは、やめなければならない。

#### 「反社会的なAI」から見えてきたこと

AIのシステムはたいてい、将来的に起こりうることを正確に予測するための学習に、大量のデータセットを必要とする。例えば、皮膚がんの兆候を検出するようなAIであれば、その利益は明らかだ。

マサチューセッツ工科大学（MIT）メディアラボは6月、恐らくは世界初となる[「反社会的な人格」を備えたAI](http://norman-ai.mit.edu/)を公開した。「ノーマン」と名付けられたこのAIサイコパスは、機械学習でアルゴリズムを生成する際にデータがどれだけ重要な役割を果たすかを理解してもらうためにつくられたもので、オンライン掲示板「reddit」にアップロードされた死体などの残虐な画像を使って訓練された。

研究チームは一方で、より穏やかな画像で学習した普通のAIも用意した。そして、両者にロールシャッハテストに使われるインクしみを見せて何を連想するか尋ねたところ、普通のAIが「木の枝に止まる鳥」と判断した画像を、ノーマンは「感電死した男」と描写したのだ。

AIが人間の人生を左右するような決定を下す場合、社会的にすでに不利な立場にある人々にさらなるダメージを与えるリスクが存在する。同時に、被支配者から支配者への権限の移譲も加速する。これは民主主義の原則に反している。

ニュージャージー州など一部の州では保釈金制度の見直しに向けて、容疑者を釈放すべきか判断する際にアルゴリズムが使われている。現行の制度は本来の目的通りには機能していないだけでなく、保釈金を支払えない者にとって著しく不公平だということは、複数の研究によって示されている。

つまり多くの場合において、裁判前の未決期間の自由を“買う”金がない貧困層の勾留と、「推定無罪」という原理の下で保障された権利を否定する結果につながっているのだ。

#### 構造的な問題解決の重要性

理想的には制度改革が行われるべきだろう。だが保釈金を廃止すれば、代わりに電子モニタリングや強制的な薬物検査など、金銭は伴わないものの懲罰的な措置を導入せざるを得ないのはでないかと懸念する声もある。

保釈の際に設けられる条件が、容疑者の逃亡や証拠隠滅といったリスクの軽減において効果的なのかということは、現状ではほとんどわかっていない。当然のように、強制薬物検査やGPSによる監視などの条件付きでの保釈が、容疑者本人や社会にどのような影響を及ぼすのかという重要な質問に対する答えも見つかっていない。例えば、保釈中にGPS付きの足輪をはめなければならない場合、それは就業の妨げとなるのだろうか。

こうしたことを考えたとき、仮に制度改革を実施しても、結局は保釈金と似たり寄ったりの有害なシステムを構築するだけで終わってしまう可能性はある。わたしたちは社会システムを改良する機会を失うのだ。

これを避けるには、旧来のモデルに機械学習という新しい技術を適用して社会的弱者を罰することを続けるのではなく、貧困と犯罪の根底にある構造的な問題を探ることに注力する必要がある。

これは何も刑事司法制度に限った話ではない。ニューヨーク州立大学オールバニ校の政治学者ヴァージニア・ユーバンクスは、今年1月に出版された『Automating Inequality』（自動化された不平等）で、アルゴリズムを使った意思決定システムの失敗例をいくつか紹介している。なかでも胸が痛むのは、ペンシルヴェニア州アレゲニー郡の自治体が導入した、児童虐待に関する相談電話のモニタリングにデータを活用する事例だ。

具体的には、ケースワーカーが介入して児童を保護すべきか判断する際に、その判断を支援するためのアルゴリズムが作成された。ここでは公的機関のデータが用いられたが、過去に虐待が認められたのは貧困層の子供が多かったため、結果として基本的に貧困層の子供を「ハイリスク」と識別すようなアルゴリズムができてしまった。

虐待のひとつにネグレクト（育児放棄）があるが、ネグレクトの危険性を示す兆候は貧困層の児童に見られる特徴と重なる部分が多い。例えば、極度の貧困で紙おむつを買えないからといって、それは育児の放棄には当たらない。しかし、アルゴリズムに判断を任せれば、そういった状況が虐待の恐れがあると見なされ、子供が公共の保護施設に入れられることが起こり得るのだ。

#### ものごとの因果関係の理解に努めるべき

ユーバンクスはデータやアルゴリズムを、貧困を引き起こす原因を探るために活用してはどうかと提案する。機械的に子供を親から引き離すのではなく、AIには「家庭を安定させるために最も効果的な手段は何か」といった質問をすべきだというのだ。

MITのチェルシー・バラバスは2月に行われたカンファレンスで、わたしも執筆に関わった論文「Interventions over Predictions: Reframing the Ethical Debate for Actuarial Risk Assessment」（予測への介入：リスク評価における倫理的議論の再構築）の[発表を行った](https://www.youtube.com/watch?v=WV54CQiplSI)。カンファレンスでは、社会システムにテクノロジーを応用する際に、公平性や信頼性、透明性といったことをどのように確保していくかが話し合われた。

わたしたちはこの論文で、テック業界はAIの倫理的な危険性を判断するうえで間違った基準を使ってきたという主張を展開している。正確さや偏向という観点から考えたとき、AIの利点もリスクも限定的にしか捉えられていない。結果として、わたしたちはオートメーションやプロファイリング、予測モデルといったものにAIを導入することで、社会的にはどのようなメリットがあるのかという基本的な問いに答えることをおろそかにしてしまったのだ。

この議論をやり直すためには、「偏りのない」システムの作成はひとまず置いて、先にものごとの因果関係を理解することに努めるべきだ。保釈された容疑者が決められた日に出廷しない理由は何だろう。赤ん坊がおむつを換えてもらえないのはどうしてだろうか。

公共サーヴィスの管理運営にアルゴリズムを的確に活用することで、効果的な社会支援システムを設計することが可能になるが、ここには既存の不平等の固定化という大きなリスクもある。メディアラボのプロジェクトのひとつである「[Humanizing AI Law](https://www.media.mit.edu/projects/HAL/overview/)（HAL）」は、この問題に焦点を当てている。ほかにもまだ数は少ないが、社会科学やコンピューターサイエンスの専門家たちを巻き込んで、同様の試みが行われるようになっている。

予測モデルが無益だとは思わないし、因果関係がわかればすべてが解決するわけでもない。社会的な問題に取り組むのは骨の折れる仕事だ。

わたしがここで言いたいのは、大量のデータはいま目の前で実際に何が起きているのかを理解するために使われるべきだ、ということだ。焦点を変えることで、より平等でさまざまな機会に満ちた社会を構築することが可能になるかもしれない。そして『マイノリティ・リポート』のような悪夢は避けることができるのではないだろうか。

###### Credits

Translation by Chihiro OKA